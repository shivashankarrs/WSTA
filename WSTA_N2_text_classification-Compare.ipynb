{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the corpus we will be using, which is included in NLTK. You will need NLTK and Scikit-learn (as well as their dependencies, in particular scipy and numpy) to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\shivashankar\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\") # if necessary\n",
    "from nltk.corpus import reuters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK sample of the Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and is divided into a training and test sets, a split which we will preserve here. Let's look at the counts of texts of the various categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'acq', 2369)\n",
      "(u'alum', 58)\n",
      "(u'barley', 51)\n",
      "(u'bop', 105)\n",
      "(u'carcass', 68)\n",
      "(u'castor-oil', 2)\n",
      "(u'cocoa', 73)\n",
      "(u'coconut', 6)\n",
      "(u'coconut-oil', 7)\n",
      "(u'coffee', 139)\n",
      "(u'copper', 65)\n",
      "(u'copra-cake', 3)\n",
      "(u'corn', 237)\n",
      "(u'cotton', 59)\n",
      "(u'cotton-oil', 3)\n",
      "(u'cpi', 97)\n",
      "(u'cpu', 4)\n",
      "(u'crude', 578)\n",
      "(u'dfl', 3)\n",
      "(u'dlr', 175)\n",
      "(u'dmk', 14)\n",
      "(u'earn', 3964)\n",
      "(u'fuel', 23)\n",
      "(u'gas', 54)\n",
      "(u'gnp', 136)\n",
      "(u'gold', 124)\n",
      "(u'grain', 582)\n",
      "(u'groundnut', 9)\n",
      "(u'groundnut-oil', 2)\n",
      "(u'heat', 19)\n",
      "(u'hog', 22)\n",
      "(u'housing', 20)\n",
      "(u'income', 16)\n",
      "(u'instal-debt', 6)\n",
      "(u'interest', 478)\n",
      "(u'ipi', 53)\n",
      "(u'iron-steel', 54)\n",
      "(u'jet', 5)\n",
      "(u'jobs', 67)\n",
      "(u'l-cattle', 8)\n",
      "(u'lead', 29)\n",
      "(u'lei', 15)\n",
      "(u'lin-oil', 2)\n",
      "(u'livestock', 99)\n",
      "(u'lumber', 16)\n",
      "(u'meal-feed', 49)\n",
      "(u'money-fx', 717)\n",
      "(u'money-supply', 174)\n",
      "(u'naphtha', 6)\n",
      "(u'nat-gas', 105)\n",
      "(u'nickel', 9)\n",
      "(u'nkr', 3)\n",
      "(u'nzdlr', 4)\n",
      "(u'oat', 14)\n",
      "(u'oilseed', 171)\n",
      "(u'orange', 27)\n",
      "(u'palladium', 3)\n",
      "(u'palm-oil', 40)\n",
      "(u'palmkernel', 3)\n",
      "(u'pet-chem', 32)\n",
      "(u'platinum', 12)\n",
      "(u'potato', 6)\n",
      "(u'propane', 6)\n",
      "(u'rand', 3)\n",
      "(u'rape-oil', 8)\n",
      "(u'rapeseed', 27)\n",
      "(u'reserves', 73)\n",
      "(u'retail', 25)\n",
      "(u'rice', 59)\n",
      "(u'rubber', 49)\n",
      "(u'rye', 2)\n",
      "(u'ship', 286)\n",
      "(u'silver', 29)\n",
      "(u'sorghum', 34)\n",
      "(u'soy-meal', 26)\n",
      "(u'soy-oil', 25)\n",
      "(u'soybean', 111)\n",
      "(u'strategic-metal', 27)\n",
      "(u'sugar', 162)\n",
      "(u'sun-meal', 2)\n",
      "(u'sun-oil', 7)\n",
      "(u'sunseed', 16)\n",
      "(u'tea', 13)\n",
      "(u'tin', 30)\n",
      "(u'trade', 485)\n",
      "(u'veg-oil', 124)\n",
      "(u'wheat', 283)\n",
      "(u'wpi', 29)\n",
      "(u'yen', 59)\n",
      "(u'zinc', 34)\n"
     ]
    }
   ],
   "source": [
    "for category in reuters.categories():\n",
    "    print(category, len(reuters.fileids(category)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the documents in the corpus are tagged with multiple labels; in this situation, a straightforward approach is to build a classifier for each label. Let's build a classifier to distinguish the most common topic in the corpus, \"acq\" (acqusitions). First, here's some code to build a dataset in preparation for classification using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_reuters_data(topic,feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for file_id in reuters.fileids():\n",
    "        feature_dict = feature_extractor(reuters.words(file_id))   \n",
    "        feature_matrix.append(feature_dict)\n",
    "        if topic in reuters.categories(file_id):\n",
    "            classifications.append(topic)\n",
    "        else:\n",
    "            classifications.append(\"not \" + topic)\n",
    "     \n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset,classifications\n",
    "\n",
    "dataset,classifications = prepare_reuters_data(\"acq\",get_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code builds a sparse bag of words feature representation (a Python dictionary) for each text in the corpus (which is pre-tokenized) and puts it in a list; a corresponding list of correct classifications is created at the same time. The scikit-learn DictVectorizer class converts Python dictionaries into the scipy sparse matrices which Scikit-learn uses; when working with a single datset, use the fit_transform method to perform the conversion. We can look at the shape of the resulting spare matrix to see how many texts and features we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 41600)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10788 texts with 41600 features, which is a fairly large feature set. Let's set up a Random Forest classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we are using default settings for this classifier. Let's do 10-fold crossvalidation, and looking at the accuracy, recall, precision, and f1-score... (if you are using the latest version of scikit learn (0.18) you will get a depreciation warning when using cross_validation, since cross_validation is included under feature_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.940674823878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.88      0.84      0.86      2369\n",
      "    not acq       0.96      0.97      0.96      8419\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10788\n",
      "\n",
      "accuracy\n",
      "0.981368186874\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.96      0.96      0.96      2369\n",
      "    not acq       0.99      0.99      0.99      8419\n",
      "\n",
      "avg / total       0.98      0.98      0.98     10788\n",
      "\n",
      "accuracy\n",
      "0.90971449759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.85      0.71      0.78      2369\n",
      "    not acq       0.92      0.97      0.94      8419\n",
      "\n",
      "avg / total       0.91      0.91      0.91     10788\n",
      "\n",
      "accuracy\n",
      "0.821653689284\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.59      0.62      0.60      2369\n",
      "    not acq       0.89      0.88      0.88      8419\n",
      "\n",
      "avg / total       0.83      0.82      0.82     10788\n",
      "\n",
      "accuracy\n",
      "0.98155357805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.96      0.95      0.96      2369\n",
      "    not acq       0.99      0.99      0.99      8419\n",
      "\n",
      "avg / total       0.98      0.98      0.98     10788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation \n",
    "\n",
    "c = 0\n",
    "for clf in classifiers:\n",
    "    c +=1 \n",
    "    dataset, classification = prepare_reuters_data(\"acq\",get_BOW_lowered_no_stopwords)\n",
    "    if c==4:\n",
    "        dataset = dataset.todense()\n",
    "    predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "    check_results(predictions, classifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a little while to build, that is because decision trees don't scale well with large feature sets, and we are building 10 sets of 10 decision tree classifiers, one for each crossvalidation fold. Let's use see what the results look like; Scikit-Learn has build in functions to calculate accuracy and recall/precision/f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"accuracy\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this case, the classifier is not obviously biased towards a particular task, so accuracy and f-score are nearly the same. The performance is quite high, indicating that it is a fairly easy classification task. Let's try to improve performance by removing stopwords and doing lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shivashankar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def get_BOW_lowered_no_stopwords(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a gain in performance, though it is fairly modest.\n",
    "\n",
    "The default number of decision trees (n_estimators) used in the model is only 10, which is fairly low: lets see if we can find a better number (this will take a while)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, more subclassifiers improved things, though at the cost of speed. Feel free to play around more with this or another classifier to see if you can do better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
